# -*- coding: utf-8 -*-
"""
scrapers/jbsmotos.py
Scraper para JBS-Motos.pt

Site PrestaShop com busca simples.
Estrat√©gia:
1. Abrir p√°gina de pesquisa direta com query
2. Extrair produtos da p√°gina de resultados (class="product-miniature")
3. Visitar cada produto, verificar refer√™ncia
4. Validar match e extrair pre√ßo
"""
import re
from typing import Optional, List, Dict

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.validation import validate_product_match
from core.selenium_utils import get_page_html, try_accept_cookies
from config import STORE_URLS
from .base import BaseScraper, SearchResult, extract_price_from_html, parse_price_to_float


class JBSMotosScraper(BaseScraper):
    """Scraper para JBS-Motos.pt"""
    
    def __init__(self):
        super().__init__(
            name="jbsmotos",
            base_url=STORE_URLS["jbsmotos"]
        )
    
    def search_product(self, driver: webdriver.Chrome, 
                      ref_parts: List[str],
                      ref_raw: str = "") -> Optional[SearchResult]:
        """
        Busca produto no JBS Motos.
        
        Args:
            driver: WebDriver Selenium
            ref_parts: Partes normalizadas (para valida√ß√£o)
            ref_raw: Refer√™ncia original (para pesquisar com h√≠fens)
            
        Returns:
            SearchResult se encontrado, None caso contr√°rio
        """
        # Usar ref_raw se dispon√≠vel (mant√©m h√≠fens), sen√£o juntar ref_parts
        if ref_raw:
            ref_query = ref_raw
        else:
            ref_query = "".join(ref_parts)
        
        print(f"[JBS] Procurando: {ref_query}")
        
        # Abrir p√°gina de resultados
        success = self._open_search_results(driver, ref_query)
        if not success:
            print(f"[JBS] ‚ùå Falha ao abrir p√°gina de resultados")
            return None
        
        # Extrair links de produtos
        product_links = self._extract_product_links(driver)
        
        if not product_links:
            print(f"[JBS] ‚ö†Ô∏è  Nenhum produto encontrado")
            return None
        
        print(f"[JBS] Encontrados {len(product_links)} produto(s)")
        
        # Visitar cada link at√© encontrar match v√°lido
        for idx, url in enumerate(product_links[:10], 1):  # M√°ximo 10
            print(f"[JBS] [{idx}] Analisando: {url}")
            
            prod_html = get_page_html(driver, url)
            if not prod_html:
                print(f"[JBS]     ‚ùå Falha ao carregar p√°gina")
                continue
            
            # Extrair refer√™ncia da p√°gina
            page_ref = self._extract_reference(prod_html)
            
            # Extrair pre√ßo
            price_text = extract_price_from_html(prod_html)
            
            if not price_text:
                print(f"[JBS]     ‚ö†Ô∏è  Pre√ßo n√£o encontrado")
                continue
            
            print(f"[JBS]     üí∞ Pre√ßo: {price_text}")
            
            # Extrair identificadores para valida√ß√£o
            identifiers = self._extract_identifiers(prod_html, page_ref)
            
            # Validar match
            soup = BeautifulSoup(prod_html, "lxml")
            full_text = soup.get_text(" ", strip=True)
            
            validation = validate_product_match(
                our_parts=ref_parts,
                page_identifiers=identifiers,
                page_url=url,
                page_text=full_text
            )
            
            print(f"[JBS]     {'‚úÖ' if validation.is_valid else '‚ùå'} Valida√ß√£o: {validation.confidence:.2f} - {validation.match_type}")
            
            if validation.is_valid:
                return SearchResult(
                    url=url,
                    price_text=price_text,
                    price_num=parse_price_to_float(price_text),
                    validation=validation
                )
        
        print(f"[JBS] ‚ùå Nenhum produto v√°lido encontrado")
        return None
    
    def _open_search_results(self, driver: webdriver.Chrome, query: str) -> bool:
        """
        Abre p√°gina de resultados de busca.
        
        URL pattern: https://jbs-motos.pt/pt/search?controller=search&s=P-HF1595
        
        Args:
            driver: WebDriver
            query: Query de busca
            
        Returns:
            True se sucesso, False caso contr√°rio
        """
        try:
            # URL de busca (l√≠ngua portuguesa)
            search_url = f"{self.base_url}pt/search?controller=search&s={query}"
            driver.get(search_url)
            
            # Aceitar cookies se aparecerem
            try_accept_cookies(driver)
            
            # Esperar pela p√°gina carregar (produtos ou mensagem "sem resultados")
            WebDriverWait(driver, 10).until(
                lambda d: (
                    len(d.find_elements(By.CSS_SELECTOR, ".product-miniature")) > 0 or
                    "Resultados da pesquisa" in d.page_source
                )
            )
            
            return True
        
        except Exception as e:
            print(f"[JBS] ‚ùå Erro ao abrir resultados: {e}")
            return False
    
    def _extract_product_links(self, driver: webdriver.Chrome) -> List[str]:
        """
        Extrai links de produtos da p√°gina de resultados.
        
        Produtos no JBS Motos t√™m:
        - Classe "product-miniature"
        - Link dentro de <h3><a>
        
        Args:
            driver: WebDriver
            
        Returns:
            Lista de URLs (sem duplicados)
        """
        links = []
        seen = set()
        
        try:
            # Encontrar todos os produtos
            products = driver.find_elements(By.CSS_SELECTOR, ".product-miniature")
            
            for product in products:
                try:
                    # Procurar link dentro do t√≠tulo (h3 > a)
                    link_elem = product.find_element(By.CSS_SELECTOR, "h3 a")
                    href = link_elem.get_attribute("href")
                    
                    if href and href not in seen:
                        seen.add(href)
                        links.append(href)
                
                except Exception:
                    continue
        
        except Exception as e:
            print(f"[JBS] ‚ö†Ô∏è  Erro ao extrair links: {e}")
        
        return links
    
    def _extract_reference(self, html: str) -> Optional[str]:
        """
        Extrai refer√™ncia da p√°gina do produto.
        
        No JBS Motos, a refer√™ncia est√° em:
        <span itemprop="sku">P-HF1595</span>
        
        Args:
            html: HTML da p√°gina
            
        Returns:
            Refer√™ncia ou None
        """
        soup = BeautifulSoup(html, "lxml")
        
        # Procurar por itemprop="sku"
        sku_tag = soup.find(attrs={"itemprop": "sku"})
        if sku_tag:
            ref = sku_tag.get_text(strip=True)
            if ref:
                return ref.upper()
        
        return None
    
    def _extract_identifiers(self, html: str, page_ref: Optional[str] = None) -> Dict[str, List[str]]:
        """
        Extrai identificadores da p√°gina (c√≥digos, SKU, refer√™ncia).
        
        Args:
            html: HTML da p√°gina
            page_ref: Refer√™ncia extra√≠da da p√°gina (se dispon√≠vel)
            
        Returns:
            Dict {"sku": [...], "codes": [...]}
        """
        soup = BeautifulSoup(html, "lxml")
        ids = {"sku": [], "codes": []}
        
        # 1. Refer√™ncia da p√°gina (priorit√°rio)
        if page_ref:
            ids["sku"].append(page_ref)
            ids["codes"].append(page_ref)
        
        # 2. T√≠tulo da p√°gina
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)
            # Extrair c√≥digos alfanum√©ricos do t√≠tulo
            pattern = re.compile(r"\b([A-Z0-9][\w\-\.+]{2,})\b", re.I)
            for match in pattern.finditer(title):
                code = match.group(1).upper()
                from core.normalization import norm_token
                if len(norm_token(code)) >= 3:
                    ids["codes"].append(code)
        
        # 3. Meta description
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc:
            content = meta_desc.get("content", "")
            pattern = re.compile(r"\b([A-Z0-9][\w\-\.+]{3,})\b", re.I)
            for match in pattern.finditer(content):
                code = match.group(1).upper()
                from core.normalization import norm_token
                if len(norm_token(code)) >= 3:
                    ids["codes"].append(code)
        
        # 4. JSON-LD (se existir)
        import json
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string or "")
                
                def scan(obj):
                    if isinstance(obj, dict):
                        if obj.get("@type") == "Product":
                            # SKU/MPN
                            for key in ["sku", "mpn"]:
                                value = obj.get(key)
                                if value and isinstance(value, str):
                                    ids["sku"].append(value.upper())
                                    ids["codes"].append(value.upper())
                        
                        # Recurs√£o
                        for value in obj.values():
                            scan(value)
                    
                    elif isinstance(obj, list):
                        for item in obj:
                            scan(item)
                
                scan(data)
            
            except Exception:
                continue
        
        return ids
